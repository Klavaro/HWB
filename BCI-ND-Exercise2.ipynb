{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 2: Neural Networks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def logistic(h,a=1):\n",
    "    return 1/(1+np.exp(-a*h))\n",
    "def dlogistic(h,a=1):\n",
    "    return logistic(h,a)-np.power(logistic(h,a),2)\n",
    "\n",
    "signtrafunc=lambda x: (np.sign(x)+1)/2\n",
    "\n",
    "def gaussRBF(h,a=1):\n",
    "    return np.exp(-(a*h)**2)\n",
    "\n",
    "def dgaussRBF(h,a=1):\n",
    "    return -2*a*h*np.exp(-(a*h)**2)\n",
    "\n",
    "class neuron:    \n",
    "    \n",
    "    def __init__(self,w,b=0,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        self.w=np.array(w)\n",
    "        self.b=np.array(b)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "        \n",
    "    def out(self, x):\n",
    "        return self.trafunc(np.dot(self.w,x)-self.b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Gradient Descent on a single neuron (4 points - programming)\n",
    "Rewrite the neuron class given and include the Gradient Descent training algorithm as a method. Use the sum of squares error to measure the output error.\n",
    "\n",
    "Add a method calculating the last local error ($\\delta$) of the neuron and one for training the neuron using the derivative of the ouput error $\\frac{dE}{dy}=y-o$.\n",
    "Keep the last input $x$, last output $y$, last activation $h$ and last delta $\\delta$ values as object attributes, whenever they are recalculated.\n",
    "The values of $h$,$x$,$y$ and $o$ are all to be taken as those of the current training data (= their last calculation).\n",
    "\n",
    "The forumla you need is: $\\delta = f'(h_i) \\left(y-o\\right)$. \n",
    "\n",
    "The train function updates the weights by the gradient descent weight update rule $w(t+1)=w(t)-\\eta\\delta(t)\\cdot x(t)$ and the bias by $b(t+1)=b(t)-\\eta\\delta(t)$ with $\\eta$ being the learning rate. As input arguments it should get the derivative of the output error $\\frac{dE}{dy}=y-o$ and the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:    \n",
    "\n",
    "    def __init__(self,w,b,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        self.w=np.array(w)\n",
    "        self.b=np.array(b)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "        \n",
    "    def out(self, x):\n",
    "        self.lastout = self.w*x - self.b\n",
    "        return self.lastout   \n",
    "    \n",
    "    def delta(self,dE):\n",
    "        self.lastdelta = dtrafunc*(lastout - dE)\n",
    "        return self.lastdelta\n",
    "    \n",
    "    def train(self,dE,learnrate=0.1):\n",
    "        self.w = self.w - learnrate*self.lastdelta\n",
    "        self.b = self.b - learnrate*self.lastdelta\n",
    "        self.lastout = self.w*x - self.b\n",
    "        self.lastdelta = dtrafunc*(lastout - dE)\n",
    "        \n",
    "        return self.lastdelta\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: logical functions (2 points - programming)\n",
    "Train a single neuron on the logical functions below for a two-dimensional input $x$. Use instances of the neuron class above to build the equivalents to logical \"or\",\"and\" and \"xor\"-functions and test them for 2-dimensional input vectors *x* resembling all possibilities of combinations ([0,0] [1,0], [0,1], [1,1]). Do 10.000 iterations and plot the evolution of the error (the error over the iteration number). You don't need to implement a stopping criterion.\n",
    "\n",
    "Set the learning rate to $\\eta=1$ and initialize the weight $w$ and the bias $b$ randomly with normal distribution (np.random.randn).\n",
    "\n",
    "In the next cell you find an exemplary random number generator and the corresponding functions you can use for sample creation in every single iteration. In every iteration use the random input $x$, the neuron output *$y=$neuron.out($x$)* and the training data $o=yourlogicalfunction(x)$\n",
    "\n",
    "If you haven't succeeded with task 1, you can import Exercise2helperPy37 or Exercise2helperPy27 depending on your python version. The syntax of the conained neuron class is: n1=Exercise2helper.neuron(np.random.randn(2),np.random.randn(1)) for initialization and train(deltanext,weightsnext,learnrate=0.1) for training. For a single neuron as for the last layer neurons, the delta of the following neuron is replaced by the error graident of the output $\\delta_k=\\left(y-o\\right)$ and the corresponding weight is $w_k=1$ as every output neuron in the model has only one unscaled output.\n",
    "\n",
    "Repeat the training for\n",
    "\n",
    "a) the logistic function\n",
    "\n",
    "b) the gaussian Radial Basis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "bad magic number in 'Exercise2helper37': b'B\\r\\r\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-42484e6a6214>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mExercise2helper37\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: bad magic number in 'Exercise2helper37': b'B\\r\\r\\n'"
     ]
    }
   ],
   "source": [
    "import Exercise2helper37 as helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) AND function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'helper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3213ed949fa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneuron1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'helper' is not defined"
     ]
    }
   ],
   "source": [
    "neuron1=helper.neuron(np.random.randn(2),np.random.randn())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) OR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Prepare the neuron for the MLP (2 point - programming)\n",
    "Rewrite the neuron class again now preparing it to be used in an MLP, which we will actually implement next week.\n",
    "To this extent, you need to make the link between the following neurons, connected by weights $w_k$ and their delta $\\delta_k$, and the current neuron.\n",
    "\n",
    "The forumla you need is: $\\delta_i = f'(h_i) \\sum_k w_k \\delta_k$, where i is the index of the current neuron and k is the index of the following neurons (connected to it's output). For a single neuron as for the last layer neurons, the delta of the following neuron is replaced by the error graident of the output $\\delta_k=\\left(y-o\\right)$ and the corresponding weight is $w_k=1$ as every output neuron in the model has only one unscaled output.\n",
    "\n",
    "The train function updates the weights again by the gradient descent weight update rule $w(t+1)=w(t)-\\eta\\delta_i(t)\\cdot x(t)$ and the bias by $b(t+1)=b(t)-\\eta\\delta_i(t)\\cdot$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class neuron:    \n",
    "    \n",
    "    def __init__(self,w,b,trafunc=logistic,dtrafunc= dlogistic ):\n",
    "        self.w=np.array(w)\n",
    "        self.b=np.array(b)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "    def out(self, x):\n",
    "        return self.lastout\n",
    "    \n",
    "    def delta(self,deltanext,weightsnext):\n",
    "    \n",
    "        return self.lastdelta\n",
    "    \n",
    "    def train(self,deltanext,weightsnext,learnrate=0.1):\n",
    "\n",
    "        return self.lastdelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: MLP layer (3 points - programming)\n",
    "Use the class \"neuron\" to construct a neural layer class \"MLPlayer\" for a Multi-Layer Perceptron (MLP).\n",
    "It should contain a list \"MLPlayer.nodes\" which is a list of the single neurons. \n",
    "Also, there should be a method \"MLPlayer.out(x)\" that returns the outputs of the single neurons as a list for the different neurons' current weights and bias of the input vector \"x\". Initialize the weights and the biases of the single neurons randomly with normal distribution by default (np.random.randn()).\n",
    "\n",
    "Include a method \"MLPlayer.train(deltanext,W,learnrate)\" which iterates the training over the nodes by calling their \"train()\" method with the deltas of the nextlayers and the corresponding weights. W is matrix consisiting of the weights of of all neurons in the next layer. It should return the deltas and weight matrix W of the current layer.\n",
    "\n",
    "The number of outputs is equivalent to the number of nodes in the layer and the number of inputs corresponds with the number of weights per neuron. The number of neurons and the number of weights per neuron should be passed to the initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPlayer:\n",
    "    \n",
    "    def __init__(self,NodeNo,WeightNo,weightinit=np.random.randn,biasinit=np.random.randn,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        \n",
    "    def out(self,x):\n",
    "        \n",
    "    def train(self,deltanext,W,learnrate=0.1):        \n",
    "\n",
    "        return deltas, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Train an MLP on the XOR (4 points - programming)\n",
    "Train a Multi-Layer-Perceptron on the logical \"xor\"-function.Do 10.000 iterations and plot the evolution of the error. You don't need to implement a stopping criterion. Use the logistic function.\n",
    "\n",
    "Set the learning rate to $\\eta=1$.\n",
    "\n",
    "Investigate the following steps:\n",
    "\n",
    "a) The network should consist of two layers, where the first has the two input neurons and the second only one output neuron. Does it always converge?\n",
    "\n",
    "b) The network should consist of two layers, where the first has the three  input neurons and the second only one output neuron. Does it now always converge?\n",
    "\n",
    "What can we learn from this?\n",
    "\n",
    "The MLP is already implemented in the helper functions Exercise2helperPy37 or Exercise2helperPy27 (depending on your python version 3.6 or 2.7). In the next task sheet, we will actually implement it ourselves, so if you have free time, you can of course already do that now!\n",
    "The syntax of the contained MLP class is:\n",
    "*NeuralNetwork=MLP(NoInputs,ListNoNeuronsPerLayer)*\n",
    "for initialization and \n",
    "*errors=NeuralNetwork.train(NoIterations,x, o ,learnrate)* for training. $x$ and $o$ can either be function pointers as defined or arrays of samples. If they are functions, $x$ has to produce a random array of inputs of size [NoInputs,] and $o(x)$ has to produce the corresponding target function output. If they are samples they have to have the shape *x.shape=[NoIterations,NoInputs]* and *o.shape=[NoIterations,NoOutputs]*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "LayerNos=[2,1]\n",
    "NeuralNetwork=helper.MLP(2,LayerNos)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
